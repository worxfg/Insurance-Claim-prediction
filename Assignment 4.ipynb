{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83452882-ca86-4235-a079-9c67609ea94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('freMTPL2freq.csv', sep=';', decimal = ',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7ccdf8d-76b7-4fb6-a83b-13232d2f7478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process features\n",
    "data['VehPower'] = np.log(data['VehPower'])\n",
    "\n",
    "bins = [0, 6, 13, float('inf')]  \n",
    "labels = ['smallest', 'mid', 'largest'] \n",
    "data['VehAge'] = pd.cut(data['VehAge'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "data['Log_DrivAge'] = np.log(data['DrivAge'])\n",
    "data['Log_BonusMalus'] = np.log(data['BonusMalus'])\n",
    "data['Log_Density'] = np.log(data['Density'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46481cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3196100097097818, Train MAE: 0.1100268541911783, Train MSE: 0.04469596952596384\n",
      "Test Loss: 0.31591100995787774, Test MAE: 0.10932676366083637, Test MSE: 0.043524846299371193\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "ds_ct_variables = ['VehPower', 'DrivAge', 'BonusMalus', 'Density', ]\n",
    "X = data.drop(columns = ['ClaimNb', 'Exposure'])\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "Y = data['ClaimNb'] / data['Exposure']\n",
    "Exposure = data['Exposure']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, Y_train, Y_test, exposure_train, exposure_test = train_test_split( \n",
    "    X, Y, Exposure, test_size=0.1\n",
    ")\n",
    "\n",
    "X_train[ds_ct_variables] = scaler.fit_transform(X_train[ds_ct_variables])\n",
    "X_test[ds_ct_variables] = scaler.fit_transform(X_test[ds_ct_variables])\n",
    "\n",
    "\n",
    "model = PoissonRegressor(alpha = 0)\n",
    "model.fit(X_train,Y_train, sample_weight=exposure_train)\n",
    "\n",
    "# Print MAE, MSE and loss on train and test data sets\n",
    "\n",
    "Y_pred_train = model.predict(X_train)\n",
    "Y_pred_test = model.predict(X_test)\n",
    "\n",
    "mae_train = mean_absolute_error(Y_train*exposure_train, Y_pred_train)\n",
    "mae_test = mean_absolute_error(Y_test*exposure_test, Y_pred_test)\n",
    "\n",
    "mse_train = mean_squared_error(Y_train*exposure_train, Y_pred_train)\n",
    "mse_test = mean_squared_error(Y_test*exposure_test, Y_pred_test)\n",
    "\n",
    "A=np.log(Y_pred_train/Y_train)\n",
    "B=np.log(Y_pred_test / Y_test)\n",
    "\n",
    "loss_train = np.sum(exposure_train * 2*(Y_pred_train - Y_train - Y_train * A)) / np.sum(exposure_train)\n",
    "loss_test = np.sum(exposure_test * 2*(Y_pred_test - Y_test - Y_test * B)) / np.sum(exposure_test)\n",
    "\n",
    "print(f\"Train Loss: {loss_train}, Train MAE: {mae_train}, Train MSE: {mse_train}\")\n",
    "print(f\"Test Loss: {loss_test}, Test MAE: {mae_test}, Test MSE: {mse_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5788a4f6-6b39-4e71-ba23-865704457fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:34<00:00,  1.70s/it, loss=0.465]\n",
      "100%|██████████| 20/20 [00:32<00:00,  1.62s/it, loss=0.467]\n",
      "100%|██████████| 20/20 [00:32<00:00,  1.60s/it, loss=0.463]\n",
      "100%|██████████| 20/20 [00:31<00:00,  1.60s/it, loss=0.467]\n",
      "100%|██████████| 20/20 [00:31<00:00,  1.58s/it, loss=0.466]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:32<00:00,  1.61s/it, loss=0.461]\n",
      "100%|██████████| 20/20 [00:32<00:00,  1.64s/it, loss=0.462]\n",
      "100%|██████████| 20/20 [00:31<00:00,  1.59s/it, loss=0.458]\n",
      "100%|██████████| 20/20 [00:35<00:00,  1.77s/it, loss=0.463]\n",
      "100%|██████████| 20/20 [00:31<00:00,  1.57s/it, loss=0.459]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:32<00:00,  1.62s/it, loss=0.463]\n",
      "100%|██████████| 20/20 [00:32<00:00,  1.64s/it, loss=0.464]\n",
      "100%|██████████| 20/20 [00:32<00:00,  1.61s/it, loss=0.462]\n",
      "100%|██████████| 20/20 [00:32<00:00,  1.60s/it, loss=0.464]\n",
      "100%|██████████| 20/20 [00:32<00:00,  1.61s/it, loss=0.463]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:31<00:00,  1.58s/it, loss=0.457]\n",
      "100%|██████████| 20/20 [00:31<00:00,  1.60s/it, loss=0.457]\n",
      "100%|██████████| 20/20 [00:32<00:00,  1.64s/it, loss=0.453]\n",
      "100%|██████████| 20/20 [00:32<00:00,  1.63s/it, loss=0.457]\n",
      "100%|██████████| 20/20 [00:31<00:00,  1.58s/it, loss=0.455]\n"
     ]
    }
   ],
   "source": [
    "# Neural network implementation\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "nh1 = 64\n",
    "nh2 = 64\n",
    "nh3 = 32\n",
    "\n",
    "n_epochs1 = 20\n",
    "n_epochs2 = 100\n",
    "nbfolds=5\n",
    "nbmodels = 4\n",
    "lr = [1e-3,1e-3,1e-2,1e-2]\n",
    "w= [1e-2, 1e-3, 1e-2, 1e-3]\n",
    "batch_size = 10000\n",
    "        \n",
    "\n",
    "class CustomNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_size, nh1, nh2, nh3):\n",
    "        super(CustomNetwork, self).__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, nh1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(nh1, nh2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(nh2, nh3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(nh3, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x=torch.clamp(x, max=1e10) #x can be at max 20 so that then the exponential doesn't explode (especially at the first iterations)\n",
    "        return torch.exp(x)  # Apply the exponential activation here\n",
    "\n",
    "\n",
    "\n",
    "#Loss function \n",
    "def poisson_loss(y_pred, y_true, exposure):\n",
    "\n",
    "    term1 = y_pred  # λ̂\n",
    "    term2 = y_true  # y\n",
    "    term3 = y_true * torch.log(y_pred+1e-10)  # y * log(λ̂), we add 1e-10 for numerical stability in case 0 is predicted\n",
    "    #it can happen to predict 0 if the last linear layer predicts a very negative number such as 1e-20, then exp of that will be seen as 0 by the computer\n",
    "    term4 = torch.where(y_true > 0, y_true * torch.log(y_true), torch.zeros_like(y_true))  # y * log(y), 0 if y = 0\n",
    "    loss = 2 * (term1 - term2 - term3 + term4)\n",
    "    weighted_loss = torch.sum(exposure * loss) / torch.sum(exposure)  # Exposure-weighted loss\n",
    "\n",
    "    return weighted_loss\n",
    "\n",
    "\n",
    "# We do 5 fold cross validation to select the set of hyperparameters for our model (learning rate and weight decay)\n",
    "\n",
    "model_loss = []\n",
    "\n",
    "kf = KFold(n_splits=nbfolds, shuffle=True, random_state=42)\n",
    "\n",
    "for nbr in range(nbmodels):\n",
    "    print(f\"Starting model {nbr + 1}...\")\n",
    "    fold_loss = []\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
    "\n",
    "        #initialize network and optimizer for each fold and set of hyperparameters\n",
    "        network = CustomNetwork(X_train.shape[1], nh1, nh2, nh3)\n",
    "        optimizer = torch.optim.Adam(network.parameters(), lr=lr[nbr], weight_decay=w[nbr])\n",
    "        \n",
    "        Xftrain, Xftest = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        Yftrain, Yftest = Y_train.iloc[train_index.tolist()], Y_train.iloc[val_index.tolist()]\n",
    "        expftrain, expftest = exposure_train.iloc[train_index.tolist()], exposure_train.iloc[val_index.tolist()]\n",
    "        \n",
    "        X_train_torch=torch.tensor(Xftrain.astype(float).values, dtype=torch.float32)\n",
    "        Y_train_torch=torch.tensor(Yftrain.astype(float).values, dtype=torch.float32).view(-1,1)\n",
    "        X_test_torch=torch.tensor(Xftest.astype(float).values, dtype=torch.float32)\n",
    "        Y_test_torch=torch.tensor(Yftest.astype(float).values, dtype=torch.float32).view(-1,1)\n",
    "        exposure_train_torch=torch.tensor(expftrain.astype(float).values, dtype=torch.float32).view(-1,1)\n",
    "        exposure_test_torch=torch.tensor(expftest.astype(float).values, dtype=torch.float32).view(-1,1)\n",
    "    \n",
    "        train_dataset = TensorDataset(X_train_torch, Y_train_torch, exposure_train_torch)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, persistent_workers=True)\n",
    "    \n",
    "        training_loop = tqdm(range(n_epochs1)) \n",
    "        \n",
    "        for epoch in training_loop:\n",
    "            epoch_loss = 0\n",
    "            network.train()\n",
    "            for (x,y,e) in train_loader:\n",
    "    \n",
    "                optimizer.zero_grad()\n",
    "                batch_loss = poisson_loss(network(x), y, e)\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += batch_loss.item()\n",
    "        \n",
    "            # Average loss per batch\n",
    "            epoch_loss /= len(train_loader)\n",
    "            training_loop.set_postfix(loss=epoch_loss)\n",
    "            \n",
    "         # Validation loop\n",
    "        # Calculate loss for the fold\n",
    "        network.eval()\n",
    "        with torch.no_grad():\n",
    "            loss=poisson_loss(network(X_test_torch), Y_test_torch, exposure_test_torch)\n",
    "        \n",
    "        fold_loss.append(loss.item())\n",
    "    #average the fold losses for each model\n",
    "    model_loss.append(np.sum(fold_loss)/nbfolds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9849483-3729-4efd-a32a-bd7ecae3d2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4588212728500366, 0.45560272932052615, 0.4595827341079712, 0.45455277562141416]\n",
      "The best model is the number 4 with learning rate 0.01 and L2 weight 0.001\n"
     ]
    }
   ],
   "source": [
    "print(model_loss)\n",
    "minval=min(model_loss)\n",
    "mind=model_loss.index(minval)\n",
    "print(f\"The best model is the number {mind + 1} with learning rate {lr[mind]} and L2 weight {w[mind]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e079005-1519-4a5d-a5ac-4f8190a17ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [04:44<00:00,  2.85s/it, loss=0.455]\n"
     ]
    }
   ],
   "source": [
    "#We train our model on the full train data using the best hyperparameters chosen by cross validation\n",
    "n_epochs2 = 100\n",
    "network = CustomNetwork(X_train.shape[1], nh1, nh2, nh3)\n",
    "opt1 = torch.optim.Adam(network.parameters(), lr=lr[mind], weight_decay=w[mind])\n",
    "\n",
    "X_train_torch=torch.tensor(X_train.astype(float).values, dtype=torch.float32)\n",
    "Y_train_torch=torch.tensor(Y_train.astype(float).values, dtype=torch.float32).view(-1,1)\n",
    "X_test_torch=torch.tensor(X_test.astype(float).values, dtype=torch.float32)\n",
    "Y_test_torch=torch.tensor(Y_test.astype(float).values, dtype=torch.float32).view(-1,1)\n",
    "exposure_train_torch=torch.tensor(exposure_train.astype(float).values, dtype=torch.float32).view(-1,1)\n",
    "exposure_test_torch=torch.tensor(exposure_test.astype(float).values, dtype=torch.float32).view(-1,1)\n",
    "    \n",
    "train_dataset = TensorDataset(X_train_torch, Y_train_torch, exposure_train_torch)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, persistent_workers=True)\n",
    "    \n",
    "training_loop = tqdm(range(n_epochs2)) \n",
    "        \n",
    "for epoch in training_loop:\n",
    "    epoch_loss = 0\n",
    "    network.train()\n",
    "    for (x,y,e) in train_loader:\n",
    "    \n",
    "        opt1.zero_grad()\n",
    "        batch_loss = poisson_loss(network(x), y, e)\n",
    "        batch_loss.backward()\n",
    "        opt1.step()\n",
    "                \n",
    "        epoch_loss += batch_loss.item()\n",
    "        \n",
    "    # Average loss per batch\n",
    "    epoch_loss /= len(train_loader)\n",
    "    training_loop.set_postfix(loss=epoch_loss)\n",
    "            \n",
    "network.eval()\n",
    "with torch.no_grad():\n",
    "    train_loss=poisson_loss(network(X_train_torch), Y_train_torch, exposure_train_torch)\n",
    "    test_loss=poisson_loss(network(X_test_torch), Y_test_torch, exposure_test_torch)\n",
    "    mae_train=mean_absolute_error(Y_train*exposure_train, network(X_train_torch))\n",
    "    mae_test=mean_absolute_error(Y_test*exposure_test, network(X_test_torch))\n",
    "    mse_train=mean_squared_error(Y_train*exposure_train, network(X_train_torch))\n",
    "    mse_test=mean_squared_error(Y_test*exposure_test, network(X_test_torch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c64d174-15cf-4b60-a08e-5cd463726140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4556354284286499, Train MAE: 0.12242735336229696, Train MSE: 0.04531353369204714\n",
      "Test Loss: 0.455142617225647, Test MAE: 0.12240879695199187, Test MSE: 0.045381144854904874\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Loss: {train_loss}, Train MAE: {mae_train}, Train MSE: {mse_train}\")\n",
    "print(f\"Test Loss: {test_loss}, Test MAE: {mae_test}, Test MSE: {mse_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b24141f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Results:\n",
      "Best min_impurity_decrease: 0.01\n",
      "Train Loss: 0.47755958040568236, Train MAE: 0.10711256181518547, Train MSE: 0.043123485555150075\n",
      "Test Loss: 0.4766356259477309, Test MAE: 0.10706989794660637, Test MSE: 0.04316239876776954\n"
     ]
    }
   ],
   "source": [
    "# Implement a regression tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "tree_model = DecisionTreeRegressor(random_state=42)\n",
    "param_grid = {'min_impurity_decrease': [0, 0.01, 0.05, 0.1, 0.2]}\n",
    "grid_search = GridSearchCV(tree_model, param_grid, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, Y_train, sample_weight=exposure_train)\n",
    "best_tree_model = grid_search.best_estimator_\n",
    "\n",
    "# Cross-validation\n",
    "Y_pred_train_tree = best_tree_model.predict(X_train)\n",
    "Y_pred_test_tree = best_tree_model.predict(X_test)\n",
    "\n",
    "mae_train_tree = mean_absolute_error(Y_train * exposure_train, Y_pred_train_tree)\n",
    "mae_test_tree = mean_absolute_error(Y_test * exposure_test, Y_pred_test_tree)\n",
    "mse_train_tree = mean_squared_error(Y_train * exposure_train, Y_pred_train_tree)\n",
    "mse_test_tree = mean_squared_error(Y_test * exposure_test, Y_pred_test_tree)\n",
    "\n",
    "A_tree = np.log(Y_pred_train_tree / Y_train)\n",
    "A_tree[np.isinf(A_tree)] = 0\n",
    "B_tree = np.log(Y_pred_test_tree / Y_test)\n",
    "B_tree[np.isinf(B_tree)] = 0\n",
    "\n",
    "loss_train_tree = np.sum(exposure_train * 2 * (Y_pred_train_tree - Y_train - Y_train * A_tree)) / np.sum(exposure_train)\n",
    "loss_test_tree = np.sum(exposure_test * 2 * (Y_pred_test_tree - Y_test - Y_test * B_tree)) / np.sum(exposure_test)\n",
    "\n",
    "# Print MAE, MSE and loss on train and test data sets\n",
    "print(\"Decision Tree Results:\")\n",
    "print(f\"Best min_impurity_decrease: {grid_search.best_params_['min_impurity_decrease']}\")\n",
    "print(f\"Train Loss: {loss_train_tree}, Train MAE: {mae_train_tree}, Train MSE: {mse_train_tree}\")\n",
    "print(f\"Test Loss: {loss_test_tree}, Test MAE: {mae_test_tree}, Test MSE: {mse_test_tree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abb85291-ba3b-4edd-940c-d88192f338be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results:\n",
      "Best Parameters: {'max_features': 'sqrt', 'min_impurity_decrease': 0.01}\n",
      "Train Loss: 0.4775595808183377, Train MAE: 0.10711766784912458, Train MSE: 0.04312386800791132\n",
      "Test Loss: 0.4766356098079722, Test MAE: 0.10707500484963621, Test MSE: 0.04316278181876905\n"
     ]
    }
   ],
   "source": [
    "# Implement a random forest model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "param_grid = {'min_impurity_decrease': [0, 0.01, 0.05],'max_features': ['sqrt', 'log2', 0.8]}  \n",
    "grid_search = GridSearchCV(rf_model, param_grid, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, Y_train, sample_weight=exposure_train)\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Cross-validation\n",
    "Y_pred_train_rf = best_rf_model.predict(X_train)\n",
    "Y_pred_test_rf = best_rf_model.predict(X_test)\n",
    "\n",
    "mae_train_rf = mean_absolute_error(Y_train * exposure_train, Y_pred_train_rf)\n",
    "mae_test_rf = mean_absolute_error(Y_test * exposure_test, Y_pred_test_rf)\n",
    "\n",
    "mse_train_rf = mean_squared_error(Y_train * exposure_train, Y_pred_train_rf)\n",
    "mse_test_rf = mean_squared_error(Y_test * exposure_test, Y_pred_test_rf)\n",
    "\n",
    "A_rf = np.log(Y_pred_train_rf / Y_train)\n",
    "A_rf[np.isinf(A_rf)] = 0\n",
    "B_rf = np.log(Y_pred_test_rf / Y_test)\n",
    "B_rf[np.isinf(B_rf)] = 0\n",
    "\n",
    "loss_train_rf = np.sum(exposure_train * 2 * (Y_pred_train_rf - Y_train - Y_train * A_rf)) / np.sum(exposure_train)\n",
    "loss_test_rf = np.sum(exposure_test * 2 * (Y_pred_test_rf - Y_test - Y_test * B_rf)) / np.sum(exposure_test)\n",
    "\n",
    "# Print MAE, MSE and loss on train and test data sets\n",
    "print(\"Random Forest Results:\")\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Train Loss: {loss_train_rf}, Train MAE: {mae_train_rf}, Train MSE: {mse_train_rf}\")\n",
    "print(f\"Test Loss: {loss_test_rf}, Test MAE: {mae_test_rf}, Test MSE: {mse_test_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ee525e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosted Trees Results:\n",
      "Best Parameters: {'learning_rate': 0.1, 'n_estimators': 300}\n",
      "Train Loss: 0.4434741915445735, Train MAE: 0.10884477467562399, Train MSE: 0.044344323516349406\n",
      "Test Loss: 0.4463857177430613, Test MAE: 0.108792878544363, Test MSE: 0.04407424275252587\n"
     ]
    }
   ],
   "source": [
    "# Implement gradient boosted trees\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbt_model = GradientBoostingRegressor(random_state=42)\n",
    "param_grid = {'learning_rate': [0.01, 0.1, 0.2],'n_estimators': [100, 200, 300]}\n",
    "grid_search = GridSearchCV(gbt_model, param_grid, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, Y_train, sample_weight=exposure_train)\n",
    "best_gbt_model = grid_search.best_estimator_\n",
    "\n",
    "# Cross-validation\n",
    "Y_pred_train_gbt = np.clip(best_gbt_model.predict(X_train), 1e-10, None)\n",
    "Y_pred_test_gbt = np.clip(best_gbt_model.predict(X_test), 1e-10, None)\n",
    "\n",
    "mae_train_gbt = mean_absolute_error(Y_train * exposure_train, Y_pred_train_gbt)\n",
    "mae_test_gbt = mean_absolute_error(Y_test * exposure_test, Y_pred_test_gbt)\n",
    "\n",
    "mse_train_gbt = mean_squared_error(Y_train * exposure_train, Y_pred_train_gbt)\n",
    "mse_test_gbt = mean_squared_error(Y_test * exposure_test, Y_pred_test_gbt)\n",
    "\n",
    "A_gbt = np.log((Y_pred_train_gbt + 1e-10) / (Y_train + 1e-10))\n",
    "A_gbt[np.isinf(A_gbt)] = 0\n",
    "B_gbt = np.log((Y_pred_test_gbt + 1e-10) / (Y_test + 1e-10))\n",
    "B_gbt[np.isinf(B_gbt)] = 0\n",
    "\n",
    "loss_train_gbt = np.sum(exposure_train * 2 * (Y_pred_train_gbt - Y_train - Y_train * A_gbt)) / np.sum(exposure_train)\n",
    "loss_test_gbt = np.sum(exposure_test * 2 * (Y_pred_test_gbt - Y_test - Y_test * B_gbt)) / np.sum(exposure_test)\n",
    "\n",
    "\n",
    "# Print MAE, MSE and loss on train and test data sets\n",
    "print(\"Gradient Boosted Trees Results:\")\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Train Loss: {loss_train_gbt}, Train MAE: {mae_train_gbt}, Train MSE: {mse_train_gbt}\")\n",
    "print(f\"Test Loss: {loss_test_gbt}, Test MAE: {mae_test_gbt}, Test MSE: {mse_test_gbt}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
